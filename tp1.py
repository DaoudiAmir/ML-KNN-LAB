# %% [markdown]
# # TP1: Algorithme des plus proches voisins

# %% [markdown]
# ## 1) Analyse des donnÃ©es

# %%
import numpy as np
import plotly
import matplotlib 
import sklearn
from matplotlib import pyplot

# %%
data = np.loadtxt('dataset.dat', skiprows=1)

# %%
data

# %%
#sÃ©parer les observations et les labels 

X = data[:,0:2] 
y = data[:,2] 
y = y.astype(int)

# %%
from matplotlib import pyplot 
colors = np.array([x for x in "rgbcmyk"]) 
pyplot.scatter(X[:, 0], X[:, 1], color=colors[y].tolist(), s=10) 
pyplot.show()


# %%
#partition des donnÃ©es
from sklearn import model_selection
X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, train_size=0.7, 
test_size=0.3)
#choisir et entraÃ®ner un modÃ¨le (KNN)
from sklearn.neighbors import KNeighborsClassifier
one_NN = KNeighborsClassifier(n_neighbors=1, algorithm='brute')
one_NN.fit(X_train, y_train)
#score sur la base d'apprentissage
print('accuraccy on training set:', one_NN.score(X_train, y_train))
#matrice de confusion
from sklearn import metrics
y_pred_test = one_NN.predict(X_test)
metrics.confusion_matrix(y_test, y_pred_test)

# %%
# CrÃ©er une grille
x_min, x_max = X[:, 0].min()*1.1, X[:, 0].max()*1.1
y_min, y_max = X[:, 1].min()*1.1, X[:, 1].max()*1.1
x_h = (x_max - x_min)/50
y_h = (y_max - y_min)/50
xx, yy = np.meshgrid(np.arange(x_min, x_max, x_h),
np.arange(y_min, y_max, y_h))
Y = one_NN.predict(np.c_[xx.ravel(), yy.ravel()])
Y = Y.reshape(xx.shape)
#afficher les frontiÃ¨res/donnÃ©es d'apprentissage
pyplot.contourf(xx, yy, Y, cmap=pyplot.cm.Paired, alpha=0.8)
pyplot.scatter(X_train[:, 0], X_train[:, 1], cmap=pyplot.cm.Paired, color=colors[y_train].tolist())
pyplot.xlim(xx.min(), xx.max())
pyplot.ylim(yy.min(), yy.max())
pyplot.show()

# %%


# %% [markdown]
# ## 3)Analyse du fonctionnement de lâ€™algorithme

# %% [markdown]
# # ğŸ”¬ Protocole d'Analyse : Impact de la Taille d'Apprentissage
# 
# ## ğŸ“Œ Objectif de l'ExpÃ©rience
# 
# ### ğŸ¯ But Principal
# - Ã‰valuer l'impact de la taille des donnÃ©es d'apprentissage sur 1-NN
# - Identifier le point optimal d'apprentissage
# - Comprendre la relation taille/performance
# 
# ### ğŸ“Š ParamÃ¨tres d'Ã‰tude
# - **Algorithme** : 1-NN (k=1)
# - **DonnÃ©es** : X_train1 (base d'apprentissage)
# - **Validation** : X_test (base de test fixe)
# - **Plage d'Ã©tude** : 1% Ã  100% de X_train1
# 
# ## ğŸ” Protocole ExpÃ©rimental
# 
# ### ğŸ“ˆ Ã‰tapes d'Analyse
# 1. **PrÃ©paration des DonnÃ©es**
#    - GÃ©nÃ©rer des sous-ensembles de X_train1
#    - Ã‰chantillonnage : 1% â†’ 100%
#    - Maintenir la distribution des classes
# 
# 2. **ExpÃ©rimentation**
#    - EntraÃ®ner 1-NN sur chaque sous-ensemble
#    - Ã‰valuer sur X_test complet
#    - Mesurer la prÃ©cision
# 
# 3. **Visualisation**
#    - Tracer la courbe d'apprentissage
#    - Axe X : Taille de l'Ã©chantillon
#    - Axe Y : Taux de reconnaissance
# 
# ## ğŸ’¡ Questions de Recherche
# 
# ### ğŸ” Points d'Analyse
# 1. **Evolution de la Performance**
#    - Comment Ã©volue la prÃ©cision ?
#    - Y a-t-il des paliers ?
#    - Quand apparaÃ®t la convergence ?
# 
# 2. **Seuil de StabilitÃ©**
#    - Nombre minimal d'exemples nÃ©cessaire
#    - Point de stabilisation
#    - Rapport coÃ»t/bÃ©nÃ©fice
# 
# ### ğŸ“Š MÃ©triques Ã  Observer
# - Taux de reconnaissance
# - VariabilitÃ© des rÃ©sultats
# - Points de rupture dans la courbe
# 
# ## ğŸ¯ RÃ©sultats Attendus
# 
# ### ğŸ“ˆ Observations AnticipÃ©es
# - Progression initiale rapide
# - Plateau de performance
# - Identification du seuil optimal
# 
# ### ğŸ’­ Implications
# - Optimisation de la taille d'apprentissage
# - Compromis ressources/performance
# - Recommandations pratiques
# 

# %%
# Recharger la base de donnÃ©es aprÃ¨s le nouveau tÃ©lÃ©versement
file_path = "dataset.dat"
data = np.loadtxt(file_path)

# SÃ©parer les observations (features) et les labels
X = data[:, 0:2]  # Les deux premiÃ¨res colonnes sont les features
y = data[:, 2].astype(int)  # La derniÃ¨re colonne est le label, converti en entier

# VÃ©rification des dimensions
X.shape, y.shape


# %%
# Re-diviser la base en apprentissage (70%) et test (30%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)

# DÃ©finir les tailles des sous-ensembles de X_train1 (de 1% Ã  100%)
subset_sizes_train = np.linspace(0.01, 1.0, 20)  # 20 valeurs entre 1% et 100%
num_samples_train = [int(size * len(X_train)) for size in subset_sizes_train]  # Convertir en nombre d'exemples

# Stocker les taux de reconnaissance
accuracy_scores_train = []

for size in num_samples_train:
    # CrÃ©er un sous-ensemble de la base dâ€™apprentissage
    X_train1 = X_train[:size]
    y_train1 = y_train[:size]
    
    # Initialiser et entraÃ®ner le modÃ¨le 1-NN
    one_NN = KNeighborsClassifier(n_neighbors=1, algorithm='brute')
    one_NN.fit(X_train1, y_train1)
    
    # Ã‰valuer la performance sur la base de test complÃ¨te X_test
    accuracy = one_NN.score(X_test, y_test)
    accuracy_scores_train.append(accuracy)

# Tracer le graphe du taux de reconnaissance en fonction du nombre d'exemples d'apprentissage
plt.figure(figsize=(10, 5))
plt.plot(num_samples_train, accuracy_scores_train, marker='o', linestyle='-')
plt.xlabel("Nombre d'exemples d'apprentissage")
plt.ylabel("Taux de reconnaissance sur X_test")
plt.title("Impact du nombre d'exemples d'apprentissage sur la prÃ©cision")
plt.grid()
plt.show()

# Afficher les premiers et derniers rÃ©sultats pour analyse
accuracy_scores_train[:5], accuracy_scores_train[-5:]


# %% [markdown]
# # ğŸ“Š Analyse de l'Impact de la Taille des DonnÃ©es d'Apprentissage
# 
# ## ğŸ“ˆ Evolution de la PrÃ©cision
# 
# ### ğŸ” 1% des DonnÃ©es (2 exemples)
# - ğŸ“‰ PrÃ©cision trÃ¨s faible : ~44%
# - âš ï¸ Apprentissage insuffisant
# - âŒ GÃ©nÃ©ralisation mÃ©diocre
# 
# ### ğŸ“Š 10-20% des DonnÃ©es
# - ğŸ“ˆ Augmentation rapide de la prÃ©cision
# - ğŸ¯ Performance : ~85-91%
# - âœ¨ AmÃ©lioration significative
# 
# ### ğŸ“ˆ 70%+ des DonnÃ©es (~150 exemples)
# - ğŸ¯ Stabilisation : 82-85%
# - ğŸ“Š Plateau de performance
# - âš–ï¸ Point d'Ã©quilibre optimal
# 
# ## ğŸ” Analyse DÃ©taillÃ©e
# 
# ### ğŸ“‰ Phase Initiale (Peu d'Exemples)
# - âŒ Faible capacitÃ© de gÃ©nÃ©ralisation
# - âš ï¸ ModÃ¨le peu fiable
# - ğŸ“Š Performance insuffisante
# 
# ### ğŸ“ˆ Phase d'Apprentissage
# - âœ… AmÃ©lioration progressive
# - ğŸ“Š RÃ©duction des erreurs
# - ğŸ¯ Meilleure gÃ©nÃ©ralisation
# 
# ### ğŸ”„ Phase de Plateau
# - ğŸ“Š Seuil atteint Ã  ~70% des donnÃ©es
# - âš–ï¸ Plus d'exemples n'amÃ©liore pas significativement la performance
# - ğŸ¯ Point optimal d'efficacitÃ©
# 
# ## ğŸ’¡ Conclusions ClÃ©s
# 
# ### ğŸ“Œ Points Critiques
# 1. **DonnÃ©es Minimales**
#    - âš ï¸ 1% est insuffisant
#    - âŒ GÃ©nÃ©ralisation impossible
#    
# 2. **Zone Optimale**
#    - âœ… 70% des donnÃ©es suffisent
#    - ğŸ¯ Balance coÃ»t/performance optimale
# 
# 3. **Loi des Rendements DÃ©croissants**
#    - ğŸ“Š Au-delÃ  de 70%, gain marginal
#    - âš–ï¸ CoÃ»t additionnel non justifiÃ©
# 
# ## ğŸ¯ Recommandations
# - âœ… Utiliser au moins 20% des donnÃ©es pour un apprentissage viable
# - ğŸ¯ Viser 70% pour une performance optimale
# - ğŸ’¡ Ne pas surcharger inutilement au-delÃ  du plateau
# 

# %%
# DÃ©finir les tailles des sous-ensembles de X_test1 (de 1% Ã  100%)
subset_sizes_test = np.linspace(0.01, 1.0, 20)  # 20 valeurs entre 1% et 100%
num_samples_test = [max(1, int(size * len(X_test))) for size in subset_sizes_test]  # S'assurer que la taille min est 1

# Stocker les taux de reconnaissance
accuracy_scores_test = []

# Utilisation de toute la base d'apprentissage X_train
for size in num_samples_test:
    # CrÃ©er un sous-ensemble de la base de test
    X_test1 = X_test[:size]
    y_test1 = y_test[:size]
    
    # Initialiser et entraÃ®ner le modÃ¨le 1-NN sur toute la base dâ€™apprentissage
    one_NN = KNeighborsClassifier(n_neighbors=1, algorithm='brute')
    one_NN.fit(X_train, y_train)
    
    # Ã‰valuer la performance sur la base de test rÃ©duite X_test1
    accuracy = one_NN.score(X_test1, y_test1)
    accuracy_scores_test.append(accuracy)

# Tracer le graphe du taux de reconnaissance en fonction du nombre d'exemples de test
plt.figure(figsize=(10, 5))
plt.plot(num_samples_test, accuracy_scores_test, marker='o', linestyle='-')
plt.xlabel("Nombre d'exemples de test")
plt.ylabel("Taux de reconnaissance sur X_test1")
plt.title("Impact du nombre d'exemples de test sur la prÃ©cision")
plt.grid()
plt.show()

# Afficher les premiers et derniers rÃ©sultats pour analyse
accuracy_scores_test[:5], accuracy_scores_test[-5:]


# %% [markdown]
# # ğŸ“Š Analyse de l'Impact de k sur la PrÃ©cision
# 
# ## ğŸ¯ Valeur Optimale de k (k*)
# 
# ### âœ¨ RÃ©sultats ClÃ©s
# - ğŸ† Meilleure prÃ©cision : k* = 8
# - âš–ï¸ ReprÃ©sente l'Ã©quilibre optimal entre biais et variance
# - ğŸ¯ 8 voisins = point optimal pour la prise de dÃ©cision
# 
# ## ğŸ“ˆ Analyse des Tendances
# 
# ### ğŸ” k = 1 : Zone de Surapprentissage
# - ğŸ“‰ Biais trÃ¨s faible
# - ğŸ“ˆ Variance trÃ¨s Ã©levÃ©e
# - âš ï¸ ModÃ¨le trop spÃ©cialisÃ© aux donnÃ©es d'entraÃ®nement
# 
# ### ğŸ“Š Evolution avec k Croissant
# 1. **Phase d'AmÃ©lioration**
#    - ğŸ“ˆ La prÃ©cision augmente
#    - ğŸ¯ Tend vers k* (8)
#    - âœ… Meilleur Ã©quilibre progressif
# 
# 2. **Phase de DÃ©tÃ©rioration**
#    - ğŸ“‰ La prÃ©cision diminue aprÃ¨s k*
#    - âš ï¸ ModÃ¨le devient trop gÃ©nÃ©ral
#    - ğŸ” Augmentation du biais
# 
# ### âš ï¸ Impact d'un k Trop Grand
# - ğŸ”¸ Lissage excessif des frontiÃ¨res de dÃ©cision
# - ğŸ“‰ Perte de capacitÃ© Ã  capturer les structures complexes
# - âŒ Sous-apprentissage du modÃ¨le
# 
# ## ğŸ’¡ Conclusion
# L'analyse montre clairement que k=8 reprÃ©sente le point optimal oÃ¹ :
# - âœ… Le modÃ¨le gÃ©nÃ©ralise bien
# - âœ… Les frontiÃ¨res de dÃ©cision sont suffisamment flexibles
# - âœ… Le compromis biais-variance est optimal
# 

# %% [markdown]
# ## 4)Algorithme des k-ppv

# %%
# DÃ©terminer une valeur raisonnable pour kmax
kmax = int(np.sqrt(len(X_train)))  # Une rÃ¨gle souvent utilisÃ©e est sqrt(N) oÃ¹ N est le nombre d'exemples d'apprentissage
k_values = range(1, kmax + 1)  # Tester k de 1 Ã  kmax

# Stocker les performances
accuracy_scores_k = []

for k in k_values:
    # Initialiser et entraÃ®ner le modÃ¨le k-NN
    knn = KNeighborsClassifier(n_neighbors=k, algorithm='brute')
    knn.fit(X_train, y_train)
    
    # Ã‰valuer la performance sur la base de test
    accuracy = knn.score(X_test, y_test)
    accuracy_scores_k.append(accuracy)

# Tracer le graphe du taux de reconnaissance en fonction de k
plt.figure(figsize=(10, 5))
plt.plot(k_values, accuracy_scores_k, marker='o', linestyle='-')
plt.xlabel("Valeur de k")
plt.ylabel("Taux de reconnaissance sur X_test")
plt.title("Impact du paramÃ¨tre k sur la prÃ©cision en test")
plt.grid()
plt.show()

# Trouver la valeur optimale de k*
optimal_k = k_values[np.argmax(accuracy_scores_k)]
optimal_k


# %% [markdown]
# # ğŸ“Š Analyse de l'Impact de k sur la PrÃ©cision
# 
# ## ğŸ¯ Valeur Optimale de k (k*)
# 
# ### âœ¨ RÃ©sultats ClÃ©s
# - ğŸ† Meilleure prÃ©cision : k* = 8
# - âš–ï¸ ReprÃ©sente l'Ã©quilibre optimal entre biais et variance
# - ğŸ¯ 8 voisins = point optimal pour la prise de dÃ©cision
# 
# ## ğŸ“ˆ Analyse des Tendances
# 
# ### ğŸ” k = 1 : Zone de Surapprentissage
# - ğŸ“‰ Biais trÃ¨s faible
# - ğŸ“ˆ Variance trÃ¨s Ã©levÃ©e
# - âš ï¸ ModÃ¨le trop spÃ©cialisÃ© aux donnÃ©es d'entraÃ®nement
# 
# ### ğŸ“Š Evolution avec k Croissant
# 1. **Phase d'AmÃ©lioration**
#    - ğŸ“ˆ La prÃ©cision augmente
#    - ğŸ¯ Tend vers k* (8)
#    - âœ… Meilleur Ã©quilibre progressif
# 
# 2. **Phase de DÃ©tÃ©rioration**
#    - ğŸ“‰ La prÃ©cision diminue aprÃ¨s k*
#    - âš ï¸ ModÃ¨le devient trop gÃ©nÃ©ral
#    - ğŸ” Augmentation du biais
# 
# ### âš ï¸ Impact d'un k Trop Grand
# - ğŸ”¸ Lissage excessif des frontiÃ¨res de dÃ©cision
# - ğŸ“‰ Perte de capacitÃ© Ã  capturer les structures complexes
# - âŒ Sous-apprentissage du modÃ¨le
# 
# ## ğŸ’¡ Conclusion
# L'analyse montre clairement que k=8 reprÃ©sente le point optimal oÃ¹ :
# - âœ… Le modÃ¨le gÃ©nÃ©ralise bien
# - âœ… Les frontiÃ¨res de dÃ©cision sont suffisamment flexibles
# - âœ… Le compromis biais-variance est optimal
# 

# %%
# DÃ©finition d'une fonction pour tracer les frontiÃ¨res de dÃ©cision
from matplotlib.colors import ListedColormap

def plot_decision_boundaries(k, X_train, y_train, title):
    # Initialiser le modÃ¨le k-NN
    knn = KNeighborsClassifier(n_neighbors=k, algorithm='brute')
    knn.fit(X_train, y_train)

    # CrÃ©ation de la grille pour la visualisation
    h = .02  # Taille de la maille du grid
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))

    # PrÃ©dictions sur la grille
    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    # Affichage des frontiÃ¨res et des points
    plt.figure(figsize=(6, 5))
    plt.contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')
    plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='coolwarm', edgecolors='k', alpha=0.7)
    plt.title(f"FrontiÃ¨res de dÃ©cision pour k = {k}")
    plt.show()

# Afficher les frontiÃ¨res pour k = 1, k* et kmax
plot_decision_boundaries(1, X_train, y_train, "k = 1 (Fort variance, faible biais)")
plot_decision_boundaries(optimal_k, X_train, y_train, f"k = {optimal_k} (Ã‰quilibre optimal)")
plot_decision_boundaries(kmax, X_train, y_train, f"k = {kmax} (Fort biais, faible variance)")


# %%
# Stocker les performances sur la base d'apprentissage
accuracy_scores_train_set = []

for k in k_values:
    # Initialiser et entraÃ®ner le modÃ¨le k-NN
    knn = KNeighborsClassifier(n_neighbors=k, algorithm='brute')
    knn.fit(X_train, y_train)
    
    # Ã‰valuer la performance sur la base d'apprentissage
    accuracy = knn.score(X_train, y_train)
    accuracy_scores_train_set.append(accuracy)

# Tracer le graphe du taux de reconnaissance en fonction de k (apprentissage)
plt.figure(figsize=(10, 5))
plt.plot(k_values, accuracy_scores_train_set, marker='o', linestyle='-', label="Apprentissage", color='blue')
plt.plot(k_values, accuracy_scores_k, marker='s', linestyle='--', label="Test", color='red')
plt.xlabel("Valeur de k")
plt.ylabel("Taux de reconnaissance")
plt.title("Impact du paramÃ¨tre k sur la prÃ©cision en apprentissage et en test")
plt.legend()
plt.grid()
plt.show()

# Afficher les premiers et derniers rÃ©sultats pour analyse
accuracy_scores_train_set[:5], accuracy_scores_train_set[-5:]


# %% [markdown]
# # ğŸ¯ K-Nearest Neighbors: Decision Boundary Analysis
# 
# ## ğŸ” Analysis by K Value
# 
# ### ğŸ¯ K=1: High Variance Region
# > *Maximum Flexibility, Minimum Stability*
# 
# **ğŸ“Š Key Features**
# - ğŸ”¸ Highly complex, irregular boundaries
# - ğŸ”¸ Single-neighbor dependency
# - ğŸ”¸ Perfect training data fit
# 
# **âš ï¸ Challenges**
# - ğŸš« Extreme noise sensitivity
# - ğŸš« Poor generalization capability
# 
# **ğŸ“ Summary:** Maximum flexibility but unstable predictions
# 
# ---
# 
# ### ğŸ¯ K=8: Optimal Balance
# > *Sweet Spot between Flexibility and Stability*
# 
# **ğŸ“Š Key Features**
# - âœ… Balanced boundary smoothness
# - âœ… Reduced noise sensitivity
# - âœ… Stable predictions
# 
# **ğŸ’ª Strengths**
# - ğŸ¯ Excellent generalization
# - ğŸ¯ Robust predictions
# 
# **ğŸ“ Summary:** Optimal performance with balanced bias-variance trade-off
# 
# ---
# 
# ### ğŸ¯ K=14: High Bias Region
# > *Maximum Stability, Minimum Flexibility*
# 
# **ğŸ“Š Key Features**
# - ğŸ“‰ Over-smoothed boundaries
# - ğŸ“‰ Reduced model flexibility
# - ğŸ“‰ Detail loss at boundaries
# 
# **âš ï¸ Limitations**
# - ğŸš« Underfitting risk
# - ğŸš« Loss of important patterns
# 
# **ğŸ“ Summary:** Too stable, missing important patterns
# 
# ---
# 
# ## ğŸ“ Impact of K Selection
# 
# | K Value | Bias | Variance | Characteristic |
# |---------|------|----------|----------------|
# | Small (1-3) | â¬‡ï¸ Low | â¬†ï¸ High | Overfitting |
# | Optimal (8) | â¡ï¸ Balanced | â¡ï¸ Balanced | Best Performance |
# | Large (14+) | â¬†ï¸ High | â¬‡ï¸ Low | Underfitting |
# 
# ## ğŸ¯ Conclusion
# The optimal value K=8 achieves the perfect balance between:
# - ğŸ¯ Model flexibility
# - ğŸ¯ Prediction stability
# - ğŸ¯ Generalization capability
# 
# This provides the best trade-off between bias and variance for robust predictions.
# 

# %% [markdown]
# ### **Analyse de l'impact de \( k \) sur la prÃ©cision en apprentissage et en test (Ã‰tape 4d)**  
# 
# #### **1ï¸âƒ£ Tendance gÃ©nÃ©rale du graphique :**  
# - La **courbe bleue (apprentissage)** reprÃ©sente le taux de reconnaissance sur la base d'entraÃ®nement.  
# - La **courbe rouge (test)** reprÃ©sente le taux de reconnaissance sur la base de test.  
# 
# ğŸ”¹ **Pour \( k = 1 \) :**  
#    - **Apprentissage** : La prÃ©cision est **100%** â†’ **Le modÃ¨le mÃ©morise parfaitement les donnÃ©es**.  
#    - **Test** : La prÃ©cision est faible (~82%) â†’ **Le modÃ¨le surapprend et ne gÃ©nÃ©ralise pas bien**.  
# 
# ğŸ”¹ **Quand \( k \) augmente :**  
#    - **En apprentissage** : La prÃ©cision **diminue progressivement** car le modÃ¨le devient plus lisse et ne sâ€™adapte plus aux moindres variations.  
#    - **En test** : La prÃ©cision **augmente d'abord** (moins d'erreurs dues au bruit) mais **se stabilise autour de 85%** aprÃ¨s \( k = 8 \).  
# 
# ğŸ”¹ **Pour \( k \) trÃ¨s grand (\( k = 14 \)) :**  
#    - **Apprentissage** : La prÃ©cision reste assez Ã©levÃ©e mais **moins flexible**.  
#    - **Test** : La prÃ©cision **se stabilise**, car le modÃ¨le est devenu **trop gÃ©nÃ©raliste**.  
# 
# ---
# 
# ### **2ï¸âƒ£ Explication de la relation entre biais et variance**  
# | **Valeur de \( k \)** | **Biais** | **Variance** | **Explication** |
# |---|---|---|---|
# | **\( k = 1 \)** | TrÃ¨s faible | TrÃ¨s forte | Le modÃ¨le **mÃ©morise tout**, y compris le bruit (surapprentissage). |
# | **\( k = k^* = 8 \)** | ModÃ©rÃ© | ModÃ©rÃ© | **Bon Ã©quilibre** entre gÃ©nÃ©ralisation et prÃ©cision. |
# | **\( k = k_{max} = 14 \)** | Fort | Faible | Le modÃ¨le devient **trop gÃ©nÃ©ral** et ne capture plus assez de dÃ©tails. |
# 
# ---
# 
# ### **3ï¸âƒ£ Comparaison entre les courbes Apprentissage vs Test**
# - **Quand \( k \) est petit**, le modÃ¨le **surdÃ©veloppe** sur les donnÃ©es d'entraÃ®nement mais **ne gÃ©nÃ©ralise pas bien** sur de nouvelles donnÃ©es.  
# - **Quand \( k \) est optimal (\( k^* = 8 \))**, les performances en **test sont maximales** et les performances en apprentissage restent **Ã©levÃ©es mais rÃ©alistes**.  
# - **Quand \( k \) est trop grand**, la prÃ©cision en **apprentissage diminue**, mais celle en test **se stabilise**.
# 
# ---
# 
# ### **4ï¸âƒ£ Conclusion sur le choix de \( k \)**
# âœ… **\( k^* = 8 \) est la meilleure valeur** :  
# - **Ã‰quilibre entre biais et variance**  
# - **Meilleure gÃ©nÃ©ralisation**  
# - **Performance stable en test et apprentissage**  
# 
# ğŸ“Œ **Prochaine Ã©tape (4e) : Ã‰valuer le protocole utilisÃ© pour rÃ©gler \( k \)**  
# ğŸ’¡ **Question dâ€™analyse :**  
# - **Notre mÃ©thode est-elle satisfaisante ?**
# - **Peut-on l'amÃ©liorer (validation croisÃ©e, autre mÃ©trique) ?**  
# 
# Veux-tu qu'on analyse cette derniÃ¨re partie ? ğŸš€

# %%
from sklearn.model_selection import cross_val_score

# DÃ©finir les valeurs de k Ã  tester
k_values = range(1, kmax + 1)

# Stocker les scores de validation croisÃ©e
cv_scores = []

for k in k_values:
    knn = KNeighborsClassifier(n_neighbors=k, algorithm='brute')
    scores = cross_val_score(knn, X_train, y_train, cv=5, scoring='accuracy')  # Validation croisÃ©e 5-fold
    cv_scores.append(scores.mean())

# Trouver la meilleure valeur de k selon la validation croisÃ©e
optimal_k_cv = k_values[np.argmax(cv_scores)]

# Tracer le graphe du taux de reconnaissance en fonction de k (validation croisÃ©e vs test simple)
plt.figure(figsize=(10, 5))
plt.plot(k_values, accuracy_scores_k, marker='o', linestyle='-', label="Test unique", color='red')
plt.plot(k_values, cv_scores, marker='s', linestyle='--', label="Validation croisÃ©e", color='blue')
plt.xlabel("Valeur de k")
plt.ylabel("Taux de reconnaissance")
plt.title("Comparaison entre validation croisÃ©e et test unique")
plt.legend()
plt.grid()
plt.show()

# Comparer les rÃ©sultats
optimal_k, optimal_k_cv


# %% [markdown]
# ### **Analyse de la validation croisÃ©e vs test unique pour le choix de \( k \) (Ã‰tape 4e)**  
# 
# #### **1ï¸âƒ£ InterprÃ©tation du graphique**  
# - **La courbe rouge (test unique)** reprÃ©sente le taux de reconnaissance en test selon la mÃ©thode initiale.
# - **La courbe bleue (validation croisÃ©e 5-fold)** reprÃ©sente la moyenne des prÃ©cisions obtenues aprÃ¨s validation croisÃ©e.
# 
# ğŸ”¹ **DiffÃ©rences observÃ©es :**  
#    - La validation croisÃ©e **donne une courbe plus lisse et plus stable**.  
#    - La mÃ©thode de test unique **prÃ©sente plus de fluctuations** (elle dÃ©pend fortement de la sÃ©paration spÃ©cifique train/test).  
#    - **Pour \( k \leq 4 \)**, les deux courbes se ressemblent, mais **au-delÃ , la validation croisÃ©e donne de meilleurs rÃ©sultats globaux**.  
# 
# ğŸ”¹ **Meilleur choix de \( k \) avec la validation croisÃ©e :**  
#    - **\( k^*_{\text{test unique}} = 8 \)** (mÃ©thode initiale).  
#    - **\( k^*_{\text{validation croisÃ©e}} = 10 \)** (choix plus stable).  
#    - Avec la validation croisÃ©e, \( k^* \) est lÃ©gÃ¨rement plus Ã©levÃ©, ce qui peut **rÃ©duire la variance** sans trop augmenter le biais.
# 
# ---
# 
# #### **2ï¸âƒ£ Pourquoi la validation croisÃ©e est meilleure ?**  
# âœ… **Avantages :**
#    - **Moins dÃ©pendante d'une seule sÃ©paration train/test**.
#    - **Plus fiable pour gÃ©nÃ©raliser sur des donnÃ©es nouvelles**.
#    - **RÃ©duit l'effet de surapprentissage** en lissant les variations.
# 
# âŒ **Limites :**
#    - **Plus coÃ»teuse en temps de calcul**, car on entraÃ®ne et teste plusieurs fois.
#    - **Peut ne pas Ãªtre nÃ©cessaire si le dataset est trÃ¨s grand** (car la sÃ©paration simple est dÃ©jÃ  assez reprÃ©sentative).
# 
# ---
# 
# ### **3ï¸âƒ£ Conclusion sur le protocole de choix de \( k \)**
# | **MÃ©thode** | **SimplicitÃ©** | **FiabilitÃ©** | **StabilitÃ© des rÃ©sultats** | **CoÃ»t computationnel** |
# |-------------|--------------|--------------|-------------------|------------------|
# | **Test unique** | âœ… TrÃ¨s simple | âš ï¸ DÃ©pend du split train/test | âš ï¸ Fluctuations visibles | âœ… Rapide |
# | **Validation croisÃ©e** | âš ï¸ Plus complexe | âœ… Plus fiable | âœ… RÃ©sultats plus stables | âš ï¸ Plus lent |
# 
# ğŸ“Œ **Recommandation :**  
# - **Si le dataset est petit/modÃ©rÃ©** â†’ **Validation croisÃ©e prÃ©fÃ©rable**.  
# - **Si le dataset est trÃ¨s grand** â†’ La mÃ©thode simple peut suffire.  
# - Dans notre cas, la validation croisÃ©e donne une **meilleure estimation** et nous recommande **\( k^* = 10 \)** au lieu de **\( k^* = 8 \)**.
# 
# ---
# 
# ### **4ï¸âƒ£ Conclusion finale du choix de \( k \) pour \( k \)-NN**
# âœ… **Le protocole basÃ© sur la validation croisÃ©e est plus fiable** car :
#    - Il **gÃ©nÃ©ralise mieux** les performances du modÃ¨le.
#    - Il Ã©vite **les fluctuations dues Ã  un unique split train/test**.
#    - Il aide Ã  **choisir un \( k^* \) plus robuste**.
# 
# ğŸ’¡ **Prochaine Ã©tape possible : ExpÃ©rimenter des optimisations pour accÃ©lÃ©rer \( k \)-NN, comme KD-Tree ou Ball-Tree.**  
# 

# %%



